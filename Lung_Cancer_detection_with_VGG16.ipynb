{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5faeaa03",
   "metadata": {},
   "source": [
    "# Image Classification with Fine-Tuned VGG16\n",
    "This notebook demonstrates an end-to-end pipeline for image classification using transfer learning with the VGG16 model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01b4b9ff",
   "metadata": {},
   "source": [
    "## Import Libraries\n",
    "The first step is to import the necessary libraries. These include libraries for:\n",
    "- Data processing (`numpy`, `pandas`)\n",
    "- Visualization (`matplotlib`, `seaborn`)\n",
    "- Image handling (`skimage`)\n",
    "- Deep learning (`tensorflow`, `keras`)\n",
    "- Progress tracking (`tqdm`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "437cb000",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sn\n",
    "import skimage.io\n",
    "import os\n",
    "import tqdm\n",
    "import glob\n",
    "import tensorflow\n",
    "\n",
    "from tqdm import tqdm\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from skimage.io import imread, imshow\n",
    "from skimage.transform import resize\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import InputLayer, BatchNormalization, Dropout, Flatten, Dense, Activation, MaxPool2D, Conv2D\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.applications.vgg16 import VGG16\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from keras import optimizers\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from keras.callbacks import Callback,ModelCheckpoint,ReduceLROnPlateau\n",
    "from keras.models import Sequential,load_model\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "import keras.backend as K\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbab9e36",
   "metadata": {},
   "source": [
    "## Data Augmentation and Dataset Preparation\n",
    "Data augmentation is used to increase the variability of the training data by applying random transformations like rotation, flipping, etc. Here, we also prepare datasets for training, validation, and testing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3b6b84",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "AUTOTUNE = tf.data.experimental.AUTOTUNE\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    validation_split=0.2,\n",
    "    rotation_range=5,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2,\n",
    "    shear_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    fill_mode='nearest'\n",
    ")\n",
    "\n",
    "valid_datagen = ImageDataGenerator(rescale=1./255, validation_split=0.2)\n",
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_dataset = train_datagen.flow_from_directory(\n",
    "    directory='/content/drive/MyDrive/Deep Learning /Data/train',\n",
    "    target_size=(224, 224),\n",
    "    class_mode='categorical',\n",
    "    batch_size=64\n",
    ")\n",
    "\n",
    "valid_dataset = valid_datagen.flow_from_directory(\n",
    "    directory='/content/drive/MyDrive/Deep Learning /Data/valid',\n",
    "    target_size=(224, 224),\n",
    "    class_mode='categorical',\n",
    "    batch_size=64\n",
    ")\n",
    "\n",
    "test_dataset = test_datagen.flow_from_directory(\n",
    "    directory='/content/drive/MyDrive/Deep Learning /Data/test',\n",
    "    target_size=(224, 224),\n",
    "    class_mode='categorical',\n",
    "    batch_size=64\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f273846",
   "metadata": {},
   "source": [
    "## Model Creation\n",
    "This section builds the classification model using transfer learning. The VGG16 model is loaded with pre-trained ImageNet weights. We then add custom layers to fine-tune it for our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f198ce0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "base_model = tf.keras.applications.VGG16(input_shape=(224, 224, 3), include_top=False, weights=\"imagenet\")\n",
    "\n",
    "# Freezing layers except the last 8\n",
    "for layer in base_model.layers[:-8]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model = Sequential([\n",
    "    base_model,\n",
    "    Dropout(0.5),\n",
    "    Flatten(),\n",
    "    BatchNormalization(),\n",
    "    Dense(32, kernel_initializer='he_uniform', activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(32, kernel_initializer='he_uniform', activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.5),\n",
    "    Dense(32, kernel_initializer='he_uniform', activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dense(4, activation='softmax')\n",
    "])\n",
    "\n",
    "model.summary()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23abe156",
   "metadata": {},
   "source": [
    "## Compile and Train\n",
    "The model is compiled with the Adam optimizer and categorical cross-entropy loss. Training metrics include accuracy, precision, recall, AUC, and F1-score. We use callbacks for efficient training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "828bd8ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def f1_score(y_true, y_pred):\n",
    "    true_positives = K.sum(K.round(K.clip(y_true * y_pred, 0, 1)))\n",
    "    possible_positives = K.sum(K.round(K.clip(y_true, 0, 1)))\n",
    "    predicted_positives = K.sum(K.round(K.clip(y_pred, 0, 1)))\n",
    "    precision = true_positives / (predicted_positives + K.epsilon())\n",
    "    recall = true_positives / (possible_positives + K.epsilon())\n",
    "    f1_val = 2 * (precision * recall) / (precision + recall + K.epsilon())\n",
    "    return f1_val\n",
    "\n",
    "METRICS = [\n",
    "    tf.keras.metrics.BinaryAccuracy(name='accuracy'),\n",
    "    tf.keras.metrics.Precision(name='precision'),\n",
    "    tf.keras.metrics.Recall(name='recall'),\n",
    "    tf.keras.metrics.AUC(name='auc'),\n",
    "    f1_score\n",
    "]\n",
    "\n",
    "lrd = ReduceLROnPlateau(monitor='val_loss', patience=3, verbose=1, factor=0.5, min_lr=1e-7)\n",
    "mcp = ModelCheckpoint('model.h5')\n",
    "es = EarlyStopping(verbose=1, patience=3)\n",
    "\n",
    "model.compile(optimizer=Adam(), loss='categorical_crossentropy', metrics=METRICS)\n",
    "\n",
    "history = model.fit(\n",
    "    train_dataset,\n",
    "    validation_data=valid_dataset,\n",
    "    epochs=20,\n",
    "    verbose=1,\n",
    "    callbacks=[lrd, mcp, es]\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8ef9dc0",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "Evaluate the trained model on the test dataset and display the metrics."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f5a48b62",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "model.evaluate(test_dataset, verbose=1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f0990d1",
   "metadata": {},
   "source": [
    "## Visualize Metrics\n",
    "Plot the training and validation metrics over epochs to analyze the model's performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c66cb3cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def Train_Val_Plot(acc, val_acc, loss, val_loss, auc, val_auc, precision, val_precision, f1, val_f1):\n",
    "    fig, (ax1, ax2, ax3, ax4, ax5) = plt.subplots(1, 5, figsize=(20, 5))\n",
    "    fig.suptitle(\"MODEL'S METRICS VISUALIZATION\")\n",
    "\n",
    "    ax1.plot(range(1, len(acc) + 1), acc, label='Training Accuracy')\n",
    "    ax1.plot(range(1, len(val_acc) + 1), val_acc, label='Validation Accuracy')\n",
    "    ax1.set_title('Accuracy')\n",
    "    ax1.legend()\n",
    "\n",
    "    ax2.plot(range(1, len(loss) + 1), loss, label='Training Loss')\n",
    "    ax2.plot(range(1, len(val_loss) + 1), val_loss, label='Validation Loss')\n",
    "    ax2.set_title('Loss')\n",
    "    ax2.legend()\n",
    "\n",
    "    ax3.plot(range(1, len(auc) + 1), auc, label='Training AUC')\n",
    "    ax3.plot(range(1, len(val_auc) + 1), val_auc, label='Validation AUC')\n",
    "    ax3.set_title('AUC')\n",
    "    ax3.legend()\n",
    "\n",
    "    ax4.plot(range(1, len(precision) + 1), precision, label='Training Precision')\n",
    "    ax4.plot(range(1, len(val_precision) + 1), val_precision, label='Validation Precision')\n",
    "    ax4.set_title('Precision')\n",
    "    ax4.legend()\n",
    "\n",
    "    ax5.plot(range(1, len(f1) + 1), f1, label='Training F1-Score')\n",
    "    ax5.plot(range(1, len(val_f1) + 1), val_f1, label='Validation F1-Score')\n",
    "    ax5.set_title('F1-Score')\n",
    "    ax5.legend()\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "Train_Val_Plot(\n",
    "    history.history['accuracy'], history.history['val_accuracy'],\n",
    "    history.history['loss'], history.history['val_loss'],\n",
    "    history.history['auc'], history.history['val_auc'],\n",
    "    history.history['precision'], history.history['val_precision'],\n",
    "    history.history['f1_score'], history.history['val_f1_score']\n",
    ")\n"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
